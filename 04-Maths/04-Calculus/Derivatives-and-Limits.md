# Derivatives and Limits

**ğŸ  [Main](../../README.md)** | **ğŸ§® [Mathematics](../README.md)** | **ğŸ“ˆ [Calculus](../README.md)**

Calculus is the mathematical study of continuous change, providing the foundation for optimization, machine learning, physics simulations, and algorithm analysis.

## Overview

Calculus applications in computer science:
- **Machine Learning**: Gradient descent, backpropagation, optimization
- **Computer Graphics**: Smooth animations, curve fitting, surface modeling
- **Algorithm Analysis**: Growth rates, continuous approximations
- **Numerical Methods**: Root finding, integration, differential equations
- **Game Development**: Physics engines, motion trajectories
- **Data Science**: Regression analysis, signal processing

## Limits

### Definition of a Limit

**Mathematical Definition:**
$$\lim_{x \to a} f(x) = L$$
means for every $\varepsilon > 0$, there exists $\delta > 0$ such that:
$$0 < |x - a| < \delta \Rightarrow |f(x) - L| < \varepsilon$$

**Intuitive Understanding:**
As $x$ approaches $a$, $f(x)$ approaches $L$.

**Limit Visualization:**
```
f(x) = (xÂ² - 1)/(x - 1) as x â†’ 1

     y
     â†‘
   3 +     â€¢(1,2) â† removable discontinuity
     |    â•±
   2 +   â•±
     |  â•±
   1 + â•±
     |â•±
   0 +â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     0   1   2

lim[xâ†’1] (xÂ²-1)/(x-1) = lim[xâ†’1] (x+1) = 2

Numerical approach:
x â†’ 1â»:  0.9 â†’ 1.9,  0.99 â†’ 1.99,  0.999 â†’ 1.999
x â†’ 1âº:  1.1 â†’ 2.1,  1.01 â†’ 2.01,  1.001 â†’ 2.001
Limit = 2
```

### Important Limits

**Fundamental Limits:**
$$\lim_{x \to 0} \frac{\sin x}{x} = 1$$

$$\lim_{x \to \infty} \left(1 + \frac{1}{x}\right)^x = e$$

$$\lim_{h \to 0} \frac{e^h - 1}{h} = 1$$

**L'HÃ´pital's Rule:**
If $\lim_{x \to a} f(x) = \lim_{x \to a} g(x) = 0$ or $\pm\infty$, then:
$$\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$$

**Example Visualization:**
```
lim[xâ†’0] sin(x)/x = 1

    y â†‘
    1 +   â€¢ â€¢ â€¢
      |  /     \
  0.5 + /       \
      |/         \
    0 +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     -Ï€   0   Ï€

sin(x)/x approaches 1 as x â†’ 0
This limit is fundamental to the derivative of sin(x)
```

## Derivatives

### Definition of a Derivative

**Limit Definition:**
$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

**Geometric Interpretation:**
The derivative represents the slope of the tangent line at a point.

**Physical Interpretation:**
- **Position â†’ Velocity**: $v(t) = \frac{d}{dt}s(t)$
- **Velocity â†’ Acceleration**: $a(t) = \frac{d}{dt}v(t)$

**Derivative Visualization:**
```
f(x) = xÂ²  at  x = 2

     y â†‘
   9 +        â€¢
     |       â•±|
   6 +      â•± |
     |     â•±  |
   4 +â€¢â•â•â•â•±   |  â† tangent line: y = 4x - 4
     |  â•±     |    slope = f'(2) = 4
   1 + â•±      |
     |â•±       |
   0 +â”€â”€â”€â”€â”€â”€â”€â”€+â†’ x
     0   1   2   3

f'(2) = lim[hâ†’0] [(2+h)Â² - 4]/h = lim[hâ†’0] [4h + hÂ²]/h = 4
```

### Differentiation Rules

**Basic Rules:**
$$\frac{d}{dx}[c] = 0 \quad \text{(constant rule)}$$

$$\frac{d}{dx}[x^n] = nx^{n-1} \quad \text{(power rule)}$$

$$\frac{d}{dx}[cf(x)] = c \cdot f'(x) \quad \text{(scalar multiplication)}$$

$$\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x) \quad \text{(sum rule)}$$

**Product Rule:**
$$\frac{d}{dx}[f(x) \cdot g(x)] = f'(x) \cdot g(x) + f(x) \cdot g'(x)$$

**Quotient Rule:**
$$\frac{d}{dx}\left[\frac{f(x)}{g(x)}\right] = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{[g(x)]^2}$$

**Chain Rule:**
$$\frac{d}{dx}[f(g(x))] = f'(g(x)) \cdot g'(x)$$

### Common Derivatives

**Elementary Functions:**
$$\frac{d}{dx}[\sin x] = \cos x$$
$$\frac{d}{dx}[\cos x] = -\sin x$$
$$\frac{d}{dx}[e^x] = e^x$$
$$\frac{d}{dx}[\ln x] = \frac{1}{x}$$

**Chain Rule Examples:**
```
f(x) = sin(xÂ²)
f'(x) = cos(xÂ²) Â· 2x

g(x) = e^(3x+1)  
g'(x) = e^(3x+1) Â· 3

h(x) = (xÂ² + 1)^5
h'(x) = 5(xÂ² + 1)^4 Â· 2x = 10x(xÂ² + 1)^4
```

## Applications

### Optimization

**Critical Points:**
Find where $f'(x) = 0$ or $f'(x)$ is undefined.

**Second Derivative Test:**
- If $f''(c) > 0$: local minimum
- If $f''(c) < 0$: local maximum  
- If $f''(c) = 0$: inconclusive

**Optimization Example:**
```
Minimize cost: f(x) = xÂ³ - 6xÂ² + 9x + 15

Step 1: f'(x) = 3xÂ² - 12x + 9 = 3(xÂ² - 4x + 3) = 3(x-1)(x-3)
Step 2: Critical points: x = 1, x = 3
Step 3: f''(x) = 6x - 12
        f''(1) = -6 < 0  â†’ local maximum
        f''(3) = 6 > 0   â†’ local minimum

    f(x) â†‘
      20 +     â€¢(1,19)
         |    â•± \
      15 +   â•±   \
         |  â•±     \
      10 + â•±       â€¢(3,15)
         |â•±         
       5 +           
         +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
         0   1   2   3   4

Minimum at x = 3 with value f(3) = 15
```

### Related Rates

**Problem Type:**
Given relationships between quantities and their rates of change, find unknown rates.

**Strategy:**
1. Identify the relationship equation
2. Differentiate both sides with respect to time
3. Substitute known values
4. Solve for the unknown rate

**Example:**
```
Balloon inflation: V = (4/3)Ï€rÂ³

Given: dr/dt = 2 cm/s
Find: dV/dt when r = 5 cm

Solution:
dV/dt = d/dt[(4/3)Ï€rÂ³] = (4/3)Ï€ Â· 3rÂ² Â· dr/dt = 4Ï€rÂ² Â· dr/dt

When r = 5 and dr/dt = 2:
dV/dt = 4Ï€(5)Â² Â· 2 = 200Ï€ cmÂ³/s

Visualization:
    r(t) â†‘
       5 +     â€¢
         |    â•±
       3 +   â•±  slope = dr/dt = 2
         |  â•±
       1 + â•±
         +â”€â”€â”€â”€â†’ t
         
Volume grows as V(t) = (4/3)Ï€[r(t)]Â³
Rate of volume change = 4Ï€rÂ²(dr/dt)
```

## Computational Applications

### Numerical Differentiation

**Forward Difference:**
$$f'(x) \approx \frac{f(x+h) - f(x)}{h}$$

**Central Difference (more accurate):**
$$f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$$

**Implementation Example:**
```python
import numpy as np

def numerical_derivative(f, x, h=1e-5):
    """Compute numerical derivative using central difference"""
    return (f(x + h) - f(x - h)) / (2 * h)

# Example: f(x) = xÂ²
f = lambda x: x**2
x = 2.0

analytical = 2 * x      # f'(x) = 2x
numerical = numerical_derivative(f, x)

print(f"Analytical: {analytical}")    # 4.0
print(f"Numerical:  {numerical}")     # â‰ˆ 4.0
```

### Automatic Differentiation

**Forward Mode:**
Computes derivatives alongside function evaluation.

**Reverse Mode (Backpropagation):**
Used in machine learning for gradient computation.

**Example in Machine Learning:**
```
Loss function: L(w) = (1/2)(wx - y)Â²
Gradient: âˆ‡L = (wx - y) Â· x

For optimization: w_new = w_old - Î± Â· âˆ‡L
where Î± is the learning rate.

Visualization of gradient descent:
    L(w) â†‘
         |     â€¢
      10 +    â•± \
         |   â•±   \
       5 +  â•±  â€¢  \  â† step 1
         | â•±   â€¢   \   â† step 2  
       0 +â•±_____â€¢___\â†’ w
        -2   -1  0  1  2

Converges to minimum via gradient descent
```
